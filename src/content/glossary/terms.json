[
  {
    "id": "transformer",
    "term": "Transformer",
    "shortDefinition": "A neural network architecture that processes sequences using attention mechanisms, enabling parallel computation.",
    "fullDefinition": "A Transformer is a deep learning architecture introduced in 2017 that revolutionized natural language processing. Unlike earlier models that processed text sequentially, Transformers can analyze entire sequences at once using self-attention, making them faster and more effective at understanding context.",
    "businessContext": "Transformers power most modern AI assistants and language tools, including ChatGPT, Claude, and Google's Bard. Understanding this term helps you evaluate AI products and their capabilities.",
    "inMeetingExample": "We're evaluating transformer-based solutions for our customer service automation.",
    "example": "When you use ChatGPT or Google Translate, you're using a transformer model.",
    "relatedTermIds": ["attention", "self-attention", "llm", "gpt"],
    "relatedMilestoneIds": ["E2017_TRANSFORMER"],
    "category": "model_architecture"
  },
  {
    "id": "llm",
    "term": "Large Language Model (LLM)",
    "shortDefinition": "An AI model trained on massive text data to understand and generate human-like text.",
    "fullDefinition": "A Large Language Model is an AI system trained on billions of words from the internet and books. LLMs learn patterns in language that allow them to answer questions, write content, translate languages, and have conversations. Examples include GPT-4, Claude, and Llama.",
    "businessContext": "LLMs are the technology behind AI assistants, chatbots, and content generation tools your company may be evaluating or already using.",
    "inMeetingExample": "Our team is testing several LLMs to see which best fits our content needs.",
    "example": "ChatGPT and Claude are both LLMs that can help draft emails, summarize documents, or brainstorm ideas.",
    "relatedTermIds": ["transformer", "gpt", "fine-tuning", "prompt"],
    "relatedMilestoneIds": ["E2020_GPT3", "E2022_CHATGPT"],
    "category": "core_concept"
  },
  {
    "id": "attention",
    "term": "Attention Mechanism",
    "shortDefinition": "A technique that lets AI models focus on relevant parts of input when making predictions.",
    "fullDefinition": "Attention is a mechanism that allows neural networks to weigh the importance of different parts of input data. Like how humans focus on specific words when reading, attention helps AI models determine which elements matter most for a given task.",
    "businessContext": "Attention mechanisms make AI more accurate and interpretable. Products using attention can often explain why they made certain decisions.",
    "example": "When translating 'The cat sat on the mat' to French, attention helps the model connect 'cat' with 'chat' and 'mat' with 'tapis'.",
    "relatedTermIds": ["transformer", "self-attention"],
    "relatedMilestoneIds": ["E2017_TRANSFORMER", "E2014_ATTENTION_NMT"],
    "category": "technical_term"
  },
  {
    "id": "prompt",
    "term": "Prompt",
    "shortDefinition": "The text input you give to an AI model to get a response.",
    "fullDefinition": "A prompt is the instruction or question you provide to an AI system. The quality of your prompt significantly affects the quality of the AI's response. Prompt engineering is the practice of crafting effective prompts to get better results.",
    "businessContext": "Understanding prompts is essential for getting value from AI tools. Better prompts lead to better outputs, making prompt skills valuable for any team using AI.",
    "inMeetingExample": "Let me share the prompt template that's been working well for our marketing copy.",
    "example": "Instead of asking 'Write about AI', a better prompt might be 'Write a 200-word explanation of machine learning for small business owners.'",
    "relatedTermIds": ["llm", "prompt-engineering"],
    "relatedMilestoneIds": ["E2022_CHATGPT"],
    "category": "business_term"
  },
  {
    "id": "fine-tuning",
    "term": "Fine-tuning",
    "shortDefinition": "Customizing a pre-trained AI model for a specific task or domain.",
    "fullDefinition": "Fine-tuning is the process of taking an AI model that was trained on general data and training it further on specialized data for your specific use case. This makes the model better at particular tasks without starting from scratch.",
    "businessContext": "Fine-tuning lets companies create AI that understands their industry jargon, follows their brand voice, or handles domain-specific tasks better than general-purpose models.",
    "inMeetingExample": "We could fine-tune an LLM on our customer support tickets to improve response quality.",
    "example": "A legal firm might fine-tune an LLM on legal documents to create a model that's better at legal research.",
    "relatedTermIds": ["llm", "training", "transfer-learning"],
    "relatedMilestoneIds": ["E2020_GPT3"],
    "category": "technical_term"
  },
  {
    "id": "neural-network",
    "term": "Neural Network",
    "shortDefinition": "A computing system inspired by the brain, made of interconnected nodes that learn from data.",
    "fullDefinition": "A neural network is a type of machine learning model loosely inspired by the human brain. It consists of layers of connected nodes (neurons) that process information. Neural networks learn by adjusting the strength of connections between nodes based on training data.",
    "businessContext": "Neural networks are the foundation of modern AI. Understanding them helps you grasp how AI products learn and improve over time.",
    "inMeetingExample": "The recommendation system uses a neural network trained on our customer behavior data.",
    "example": "Netflix's recommendation engine uses neural networks to predict what shows you might enjoy based on your viewing history.",
    "relatedTermIds": ["deep-learning", "machine-learning", "backpropagation"],
    "relatedMilestoneIds": ["E1943_MCCULLOCH_PITTS", "E1958_PERCEPTRON"],
    "category": "core_concept"
  },
  {
    "id": "deep-learning",
    "term": "Deep Learning",
    "shortDefinition": "Machine learning using neural networks with many layers to learn complex patterns.",
    "fullDefinition": "Deep learning is a subset of machine learning that uses neural networks with many layers (hence 'deep'). These deep networks can automatically learn hierarchical representations of data, making them excellent at tasks like image recognition, speech processing, and language understanding.",
    "businessContext": "Deep learning powers most cutting-edge AI applications. When vendors mention 'AI-powered' features, they're usually referring to deep learning technology.",
    "inMeetingExample": "Our image recognition feature uses deep learning to automatically tag product photos.",
    "example": "Face ID on your iPhone uses deep learning to recognize your face even as your appearance changes slightly over time.",
    "relatedTermIds": ["neural-network", "machine-learning", "cnn"],
    "relatedMilestoneIds": ["E2012_ALEXNET", "E2006_DEEP_BELIEF_NETS"],
    "category": "core_concept"
  },
  {
    "id": "machine-learning",
    "term": "Machine Learning",
    "shortDefinition": "AI that learns from data rather than following explicit programming rules.",
    "fullDefinition": "Machine learning is a branch of AI where systems learn patterns from data rather than being explicitly programmed. Instead of writing rules for every scenario, ML systems discover patterns in examples and use those patterns to make predictions on new data.",
    "businessContext": "Machine learning is the technology behind recommendations, fraud detection, and predictive analytics that many businesses rely on today.",
    "inMeetingExample": "We're using machine learning to predict which customers are likely to churn.",
    "example": "Spam filters use machine learning to learn which emails are spam based on examples of spam and legitimate emails.",
    "relatedTermIds": ["neural-network", "deep-learning", "training"],
    "relatedMilestoneIds": ["E1958_PERCEPTRON", "E1986_BACKPROP"],
    "category": "core_concept"
  },
  {
    "id": "backpropagation",
    "term": "Backpropagation",
    "shortDefinition": "The algorithm that allows neural networks to learn by propagating errors backward through layers.",
    "fullDefinition": "Backpropagation is the core learning algorithm for neural networks. It works by calculating how much each connection in the network contributed to an error, then adjusting those connections to reduce future errors. This process repeats millions of times during training.",
    "businessContext": "While technical, backpropagation is why modern AI can learn from data. It's the reason we can train models on examples rather than programming every rule manually.",
    "example": "When an image recognition model misidentifies a cat as a dog, backpropagation adjusts the model so it's more likely to get it right next time.",
    "relatedTermIds": ["neural-network", "training", "gradient-descent"],
    "relatedMilestoneIds": ["E1986_BACKPROP"],
    "category": "technical_term"
  },
  {
    "id": "gan",
    "term": "GAN (Generative Adversarial Network)",
    "shortDefinition": "An AI architecture where two neural networks compete to generate realistic content.",
    "fullDefinition": "A GAN consists of two neural networks: a generator that creates fake data and a discriminator that tries to tell fake from real. They improve together through competition, with the generator getting better at creating realistic outputs. GANs revolutionized AI image generation.",
    "businessContext": "GANs enabled the first wave of AI-generated images and are still used for tasks like creating synthetic training data or enhancing image resolution.",
    "inMeetingExample": "We could use a GAN to generate synthetic customer data for testing without privacy concerns.",
    "example": "StyleGAN can generate photorealistic faces of people who don't exist, used in stock photography and game character creation.",
    "relatedTermIds": ["neural-network", "diffusion-model", "generative-ai"],
    "relatedMilestoneIds": ["E2014_GANS"],
    "category": "model_architecture"
  },
  {
    "id": "diffusion-model",
    "term": "Diffusion Model",
    "shortDefinition": "An AI that generates images by gradually removing noise from random static.",
    "fullDefinition": "Diffusion models create images through a process of denoising. They start with pure random noise and gradually refine it into a coherent image, guided by a text description. This approach has proven more stable and higher quality than GANs for many image generation tasks.",
    "businessContext": "Diffusion models power tools like DALL-E, Midjourney, and Stable Diffusion. They're driving the current wave of AI image generation in creative industries.",
    "inMeetingExample": "Our design team is using diffusion model tools for rapid concept visualization.",
    "example": "Stable Diffusion uses a diffusion model to turn text descriptions like 'a cat wearing a business suit' into images.",
    "relatedTermIds": ["gan", "latent-space", "generative-ai"],
    "relatedMilestoneIds": ["E2022_LATENT_DIFFUSION", "E2022_STABLE_DIFFUSION_RELEASE"],
    "category": "model_architecture"
  },
  {
    "id": "gpt",
    "term": "GPT (Generative Pre-trained Transformer)",
    "shortDefinition": "OpenAI's family of language models that can generate human-like text.",
    "fullDefinition": "GPT stands for Generative Pre-trained Transformer. It's OpenAI's series of language models, starting with GPT-1 in 2018 and evolving through GPT-2, GPT-3, and GPT-4. Each version became dramatically more capable, with GPT-4 being multimodal (handling images and text).",
    "businessContext": "GPT models power many AI tools you might use or evaluate. Understanding the GPT family helps you understand AI capabilities and why some tools work better than others.",
    "inMeetingExample": "This tool is built on GPT-4, so it can handle more complex instructions than our previous solution.",
    "example": "ChatGPT is built on GPT-3.5 and GPT-4, allowing it to write essays, code, and answer questions conversationally.",
    "relatedTermIds": ["llm", "transformer", "chatgpt"],
    "relatedMilestoneIds": ["E2018_GPT1", "E2019_GPT2", "E2020_GPT3", "E2023_GPT4"],
    "category": "model_architecture"
  },
  {
    "id": "bert",
    "term": "BERT",
    "shortDefinition": "A transformer model from Google that understands language context bidirectionally.",
    "fullDefinition": "BERT (Bidirectional Encoder Representations from Transformers) is Google's 2018 model that revolutionized how AI understands language. Unlike earlier models that read text left-to-right, BERT considers context from both directions simultaneously, leading to much better comprehension.",
    "businessContext": "BERT powers Google Search and many enterprise NLP applications. It's often the technology behind search, classification, and question-answering features in business software.",
    "inMeetingExample": "Our search functionality uses BERT to understand the intent behind customer queries.",
    "example": "When you search Google for 'bank near river', BERT helps distinguish between a financial bank and a riverbank based on context.",
    "relatedTermIds": ["transformer", "nlp", "gpt"],
    "relatedMilestoneIds": ["E2018_BERT"],
    "category": "model_architecture"
  },
  {
    "id": "rlhf",
    "term": "RLHF (Reinforcement Learning from Human Feedback)",
    "shortDefinition": "A training technique where AI learns from human ratings of its outputs.",
    "fullDefinition": "RLHF is a technique to align AI behavior with human preferences. Human evaluators rate AI outputs (like helpful vs. unhelpful responses), and these ratings are used to train the model to produce responses humans prefer. This is key to making chatbots safe and helpful.",
    "businessContext": "RLHF is why ChatGPT feels more natural and helpful than earlier chatbots. It's a critical technique for any AI that interacts directly with users.",
    "inMeetingExample": "The model was trained with RLHF, which is why it refuses harmful requests.",
    "example": "When ChatGPT provides helpful, safe responses instead of harmful ones, that's largely due to RLHF training.",
    "relatedTermIds": ["alignment", "constitutional-ai", "fine-tuning"],
    "relatedMilestoneIds": ["E2022_INSTRUCTGPT", "E2022_CHATGPT"],
    "category": "technical_term"
  },
  {
    "id": "alignment",
    "term": "AI Alignment",
    "shortDefinition": "Ensuring AI systems behave in accordance with human values and intentions.",
    "fullDefinition": "AI alignment is the challenge of making AI systems do what humans actually want, not just what they're literally told. This includes being helpful, honest, and harmless. As AI becomes more powerful, alignment becomes increasingly important for safety.",
    "businessContext": "Alignment determines whether AI tools are trustworthy and safe for your organization. Well-aligned models are less likely to produce harmful or misleading content.",
    "inMeetingExample": "We should evaluate this vendor's alignment practices before deploying their AI.",
    "example": "An aligned AI assistant will refuse to help with illegal activities even if asked cleverly, because it understands the intent is harmful.",
    "relatedTermIds": ["rlhf", "constitutional-ai", "ai-safety"],
    "relatedMilestoneIds": ["E2022_CONSTITUTIONAL_AI", "E2022_INSTRUCTGPT"],
    "category": "core_concept"
  },
  {
    "id": "cnn",
    "term": "CNN (Convolutional Neural Network)",
    "shortDefinition": "A neural network architecture specialized for processing images and visual data.",
    "fullDefinition": "CNNs are neural networks designed to process visual information. They work by scanning images with small filters that detect features like edges, shapes, and textures. Multiple layers detect increasingly complex features, enabling tasks like image classification and object detection.",
    "businessContext": "CNNs power visual AI applications: photo tagging, quality inspection, medical imaging, and autonomous vehicles. If your product involves image analysis, it likely uses CNNs.",
    "inMeetingExample": "Our quality control system uses a CNN to detect manufacturing defects in real-time.",
    "example": "When your phone recognizes your face or Facebook auto-tags your friends in photos, CNNs are doing the work.",
    "relatedTermIds": ["neural-network", "deep-learning", "computer-vision"],
    "relatedMilestoneIds": ["E2012_ALEXNET", "E2015_RESNET", "E1998_LENET"],
    "category": "model_architecture"
  },
  {
    "id": "nlp",
    "term": "NLP (Natural Language Processing)",
    "shortDefinition": "AI technology that enables computers to understand, interpret, and generate human language.",
    "fullDefinition": "NLP is the field of AI focused on the interaction between computers and human language. It includes understanding text (like sentiment analysis), generating text (like chatbots), and translating between languages. Modern NLP is largely powered by transformers.",
    "businessContext": "NLP enables chatbots, email filters, voice assistants, and document analysis. Any AI tool that works with text uses NLP techniques.",
    "inMeetingExample": "We're implementing NLP to automatically categorize and route customer support tickets.",
    "example": "When Gmail suggests replies to emails or Amazon Alexa understands your voice commands, NLP is at work.",
    "relatedTermIds": ["llm", "transformer", "bert"],
    "relatedMilestoneIds": ["E2017_TRANSFORMER", "E2018_BERT"],
    "category": "core_concept"
  },
  {
    "id": "training",
    "term": "Training (Machine Learning)",
    "shortDefinition": "The process of teaching an AI model by exposing it to data and adjusting its parameters.",
    "fullDefinition": "Training is the process where machine learning models learn from data. During training, a model makes predictions, compares them to correct answers, and adjusts its internal parameters to improve. This process repeats millions or billions of times until the model performs well.",
    "businessContext": "Understanding training helps you evaluate AI products. Well-trained models with good data perform better. Training also determines ongoing costs and time-to-deployment.",
    "inMeetingExample": "We'll need 3 weeks to train the model on our historical data before deployment.",
    "example": "Training GPT-4 took months of processing on thousands of specialized computers, using text from much of the internet.",
    "relatedTermIds": ["fine-tuning", "backpropagation", "dataset"],
    "relatedMilestoneIds": ["E1986_BACKPROP", "E2020_SCALING_LAWS"],
    "category": "technical_term"
  },
  {
    "id": "inference",
    "term": "Inference",
    "shortDefinition": "Using a trained AI model to make predictions on new data.",
    "fullDefinition": "Inference is when a trained model is used to make predictions or generate outputs. Unlike training, which teaches the model, inference applies what the model has learned. When you ask ChatGPT a question, you're triggering inference.",
    "businessContext": "Inference costs are ongoing operational expenses for AI systems. Faster inference means better user experience. Understanding inference helps you budget for AI deployments.",
    "inMeetingExample": "Our inference costs have decreased since we optimized the model for production.",
    "example": "Every time you ask Siri a question, an inference runs on Apple's servers to generate your answer.",
    "relatedTermIds": ["training", "model", "latency"],
    "relatedMilestoneIds": ["E2022_CHATGPT"],
    "category": "technical_term"
  },
  {
    "id": "token",
    "term": "Token",
    "shortDefinition": "A piece of text (word, part of word, or character) that language models process.",
    "fullDefinition": "A token is the basic unit that language models work with. Typically, common words are single tokens, while rare words might be split into multiple tokens. Understanding tokens matters because LLM pricing and limits are often based on token counts.",
    "businessContext": "Token limits affect how much context you can provide to AI tools. Token counts determine API costs. Understanding tokens helps you estimate costs and optimize prompts.",
    "inMeetingExample": "Our document is 8,000 tokens, so we'll need to split it for processing.",
    "example": "The word 'ChatGPT' is usually 2-3 tokens, while 'the' is typically 1 token.",
    "relatedTermIds": ["llm", "prompt", "context-window"],
    "relatedMilestoneIds": ["E2020_GPT3"],
    "category": "technical_term"
  },
  {
    "id": "context-window",
    "term": "Context Window",
    "shortDefinition": "The maximum amount of text a language model can consider at once.",
    "fullDefinition": "The context window is the limit on how much text a language model can process in a single interaction. This includes both your input and the model's output. Larger context windows allow for longer documents and more complex conversations.",
    "businessContext": "Context window size is a key differentiator between AI products. Larger windows enable processing longer documents and maintaining longer conversations without 'forgetting' earlier content.",
    "inMeetingExample": "This model has a 128k token context window, so we can analyze the entire contract at once.",
    "example": "GPT-4 Turbo has a 128k token context window, roughly equivalent to a 300-page book.",
    "relatedTermIds": ["token", "llm", "prompt"],
    "relatedMilestoneIds": ["E2023_GPT4"],
    "category": "technical_term"
  },
  {
    "id": "hallucination",
    "term": "Hallucination (AI)",
    "shortDefinition": "When an AI model confidently generates false or made-up information.",
    "fullDefinition": "Hallucination occurs when AI models generate content that sounds plausible but is factually incorrect or entirely fabricated. This happens because language models generate statistically likely text, not necessarily true text. It's a major challenge for AI reliability.",
    "businessContext": "Hallucinations are a key risk when deploying AI, especially for high-stakes applications. Understanding this limitation helps you implement appropriate verification processes.",
    "inMeetingExample": "We need human review for AI-generated content to catch any hallucinations before publication.",
    "example": "An AI might confidently cite a research paper that doesn't exist, complete with author names and publication details.",
    "relatedTermIds": ["llm", "alignment", "rag"],
    "relatedMilestoneIds": ["E2020_GPT3", "E2022_CHATGPT"],
    "category": "core_concept"
  },
  {
    "id": "rag",
    "term": "RAG (Retrieval-Augmented Generation)",
    "shortDefinition": "A technique that combines AI text generation with information retrieval to improve accuracy.",
    "fullDefinition": "RAG enhances language models by having them retrieve relevant information from a knowledge base before generating responses. Instead of relying solely on what the model 'remembers' from training, RAG looks up current information, reducing hallucinations and enabling up-to-date answers.",
    "businessContext": "RAG is essential for enterprise AI applications where accuracy matters. It allows you to connect AI to your company's proprietary knowledge bases while reducing incorrect outputs.",
    "inMeetingExample": "We're implementing RAG to connect the chatbot to our product documentation.",
    "example": "When Perplexity AI searches the web and then generates an answer with citations, it's using RAG.",
    "relatedTermIds": ["llm", "hallucination", "knowledge-base"],
    "relatedMilestoneIds": ["E2020_RAG"],
    "category": "technical_term"
  },
  {
    "id": "embedding",
    "term": "Embedding",
    "shortDefinition": "A numerical representation of text or images that captures semantic meaning.",
    "fullDefinition": "An embedding converts text, images, or other data into a list of numbers that captures its meaning. Similar items have similar embeddings, enabling AI to find related content, cluster similar items, and perform semantic search rather than keyword matching.",
    "businessContext": "Embeddings power semantic search, recommendations, and RAG systems. They're how AI understands that 'automobile' and 'car' mean the same thing.",
    "inMeetingExample": "We'll use embeddings to find documents related to the user's query, even if they use different words.",
    "example": "A search using embeddings would find documents about 'automobiles' when you search for 'cars'.",
    "relatedTermIds": ["rag", "semantic-search", "vector-database"],
    "relatedMilestoneIds": ["E2013_WORD2VEC"],
    "category": "technical_term"
  },
  {
    "id": "multimodal",
    "term": "Multimodal AI",
    "shortDefinition": "AI that can understand and generate multiple types of content like text, images, and audio.",
    "fullDefinition": "Multimodal AI systems can process and generate different types of data: text, images, audio, and video. Instead of separate models for each type, multimodal models understand relationships across modalities, like describing an image in text or generating images from text.",
    "businessContext": "Multimodal capabilities are driving the next wave of AI applications. They enable richer interactions and new use cases like visual question answering and automated video analysis.",
    "inMeetingExample": "GPT-4V is multimodal, so users can upload images and ask questions about them.",
    "example": "Asking GPT-4 to analyze a chart image and explain the trends is a multimodal interaction.",
    "relatedTermIds": ["llm", "computer-vision", "diffusion-model"],
    "relatedMilestoneIds": ["E2021_CLIP", "E2021_DALLE", "E2023_GPT4"],
    "category": "core_concept"
  },
  {
    "id": "zero-shot",
    "term": "Zero-shot Learning",
    "shortDefinition": "AI performing tasks it wasn't explicitly trained for, using general knowledge.",
    "fullDefinition": "Zero-shot learning is when an AI model can perform a task without having seen specific examples of that task during training. Large language models exhibit strong zero-shot capabilities, handling new tasks based on their general understanding of language.",
    "businessContext": "Zero-shot capabilities mean you can use AI for new tasks without custom training data. This reduces time-to-value and makes AI more flexible for diverse business needs.",
    "inMeetingExample": "The model can classify these documents zero-shot; we don't need to prepare training examples.",
    "example": "Asking GPT-4 to classify customer feedback into categories without providing examples first is zero-shot learning.",
    "relatedTermIds": ["few-shot", "llm", "prompt"],
    "relatedMilestoneIds": ["E2020_GPT3"],
    "category": "technical_term"
  },
  {
    "id": "few-shot",
    "term": "Few-shot Learning",
    "shortDefinition": "AI learning to perform tasks from just a handful of examples provided in the prompt.",
    "fullDefinition": "Few-shot learning is when an AI performs tasks based on a small number of examples provided in the prompt. Instead of training on thousands of examples, you show the model 2-5 examples of what you want, and it generalizes from there.",
    "businessContext": "Few-shot learning makes AI accessible without massive training datasets. You can customize AI behavior by providing examples in your prompts rather than expensive model training.",
    "inMeetingExample": "Let me give the model a few examples of our preferred tone, and it should adapt.",
    "example": "Showing ChatGPT three examples of your email style before asking it to write emails is few-shot learning.",
    "relatedTermIds": ["zero-shot", "prompt-engineering", "llm"],
    "relatedMilestoneIds": ["E2020_GPT3"],
    "category": "technical_term"
  },
  {
    "id": "prompt-engineering",
    "term": "Prompt Engineering",
    "shortDefinition": "The practice of crafting effective prompts to get better results from AI models.",
    "fullDefinition": "Prompt engineering is the skill of designing prompts that elicit desired responses from AI models. It involves techniques like providing context, specifying format, giving examples, and breaking complex tasks into steps. Good prompts dramatically improve AI output quality.",
    "businessContext": "Prompt engineering is a practical skill that immediately improves AI tool effectiveness. Teams with strong prompt skills get significantly more value from the same AI tools.",
    "inMeetingExample": "Our prompt engineers have developed templates that consistently produce high-quality outputs.",
    "example": "Instead of 'Summarize this', a prompt-engineered version might be 'Summarize this article in 3 bullet points, focusing on business implications.'",
    "relatedTermIds": ["prompt", "few-shot", "llm"],
    "relatedMilestoneIds": ["E2022_CHATGPT", "E2020_GPT3"],
    "category": "business_term"
  },
  {
    "id": "scaling-laws",
    "term": "Scaling Laws",
    "shortDefinition": "Mathematical relationships showing AI improves predictably with more data, compute, and parameters.",
    "fullDefinition": "Scaling laws describe how AI model performance improves as you increase model size, training data, and compute. These laws showed that simply making models bigger leads to predictable improvements, guiding the development of increasingly powerful models like GPT-4.",
    "businessContext": "Scaling laws explain why bigger AI models generally perform better. They help predict future capabilities and inform strategic decisions about AI investments and timelines.",
    "inMeetingExample": "Based on scaling laws, the next model generation should be significantly more capable.",
    "example": "The jump from GPT-3 (175B parameters) to GPT-4's improved performance follows scaling law predictions.",
    "relatedTermIds": ["training", "llm", "compute"],
    "relatedMilestoneIds": ["E2020_SCALING_LAWS", "E2022_CHINCHILLA"],
    "category": "technical_term"
  },
  {
    "id": "constitutional-ai",
    "term": "Constitutional AI",
    "shortDefinition": "A technique for making AI systems follow ethical principles defined in a 'constitution'.",
    "fullDefinition": "Constitutional AI is an alignment technique developed by Anthropic. Instead of relying solely on human feedback, the AI is given a set of principles (a 'constitution') and uses self-critique to improve its responses. This makes alignment more scalable and transparent.",
    "businessContext": "Constitutional AI is behind Claude's safety features. Understanding it helps evaluate AI vendors' approaches to safety and reliability.",
    "inMeetingExample": "Anthropic uses constitutional AI to make Claude helpful but safe.",
    "example": "Claude's constitution includes principles like being helpful while avoiding harm, which guides its behavior.",
    "relatedTermIds": ["alignment", "rlhf", "ai-safety"],
    "relatedMilestoneIds": ["E2022_CONSTITUTIONAL_AI"],
    "category": "technical_term"
  },
  {
    "id": "emergent-behavior",
    "term": "Emergent Behavior",
    "shortDefinition": "Capabilities that appear in AI models unexpectedly as they grow larger.",
    "fullDefinition": "Emergent behaviors are capabilities that appear in AI models only after reaching a certain scale, without being explicitly trained. Examples include multi-step reasoning, following complex instructions, and performing tasks in ways that surprise even the model's creators.",
    "businessContext": "Emergent behaviors are both exciting and challenging. They enable new applications but also mean larger models may have unexpected capabilities that require careful evaluation.",
    "inMeetingExample": "The model's ability to write code emerged at scale; it wasn't explicitly trained for programming.",
    "example": "GPT-3's ability to perform arithmetic wasn't explicitly trained but emerged from language training at scale.",
    "relatedTermIds": ["scaling-laws", "llm"],
    "relatedMilestoneIds": ["E2020_GPT3", "E2023_GPT4"],
    "category": "technical_term"
  },
  {
    "id": "expert-system",
    "term": "Expert System",
    "shortDefinition": "AI software that mimics human expert decision-making using programmed rules.",
    "fullDefinition": "Expert systems were a dominant AI approach in the 1980s. They encode human expert knowledge as explicit if-then rules to make decisions in specific domains. Unlike modern ML, they require manually programming all the rules rather than learning from data.",
    "businessContext": "Expert systems represent an earlier AI paradigm. Understanding them helps contextualize modern AI's advantages: learning from data rather than requiring explicit programming of every rule.",
    "inMeetingExample": "Our legacy claims processing system is an expert system; we're looking to replace it with ML-based solutions.",
    "example": "MYCIN was an expert system that diagnosed bacterial infections using rules from medical experts.",
    "relatedTermIds": ["machine-learning", "rule-based"],
    "relatedMilestoneIds": ["E1980_EXPERT_SYSTEMS_RISE"],
    "category": "core_concept"
  },
  {
    "id": "ai-safety",
    "term": "AI Safety",
    "shortDefinition": "The field focused on ensuring AI systems are beneficial and don't cause harm.",
    "fullDefinition": "AI safety encompasses research and practices to ensure AI systems behave as intended, remain under human control, and don't cause unintended harm. As AI becomes more capable, safety becomes increasingly important for society and organizations deploying AI.",
    "businessContext": "AI safety isn't just ethics; it's risk management. Organizations need to consider safety when deploying AI to avoid reputational damage, legal liability, and operational failures.",
    "inMeetingExample": "We need to review the AI safety implications before deploying this system to customers.",
    "example": "Red-teaming AI systems to find potential misuse cases is an AI safety practice.",
    "relatedTermIds": ["alignment", "constitutional-ai", "rlhf"],
    "relatedMilestoneIds": ["E2018_OPENAI_CHARTER", "E2022_CONSTITUTIONAL_AI"],
    "category": "core_concept"
  },
  {
    "id": "benchmark",
    "term": "Benchmark (AI)",
    "shortDefinition": "A standardized test used to measure and compare AI model performance.",
    "fullDefinition": "AI benchmarks are standardized tests that measure model capabilities on specific tasks like question answering, coding, or math. They enable fair comparisons between models and tracking of progress over time. Common benchmarks include MMLU, HumanEval, and MATH.",
    "businessContext": "Benchmarks help you evaluate AI products objectively. However, they don't always predict real-world performance, so they're one factor among many in evaluation.",
    "inMeetingExample": "This model scores 90% on MMLU, outperforming the previous version by 5 points.",
    "example": "The MMLU benchmark tests AI on 57 subjects from math to history to law.",
    "relatedTermIds": ["evaluation", "llm"],
    "relatedMilestoneIds": ["E2009_IMAGENET"],
    "category": "technical_term"
  },
  {
    "id": "open-source-ai",
    "term": "Open Source AI",
    "shortDefinition": "AI models whose weights and code are publicly available for anyone to use and modify.",
    "fullDefinition": "Open source AI models make their trained weights, code, and sometimes training data publicly available. This allows anyone to run, modify, and build on these models. Examples include Llama, Mistral, and Stable Diffusion.",
    "businessContext": "Open source AI offers cost savings, customization, and data privacy (you can run models locally). However, it requires more technical expertise and may lag behind proprietary models in capabilities.",
    "inMeetingExample": "We could use Llama as an open source alternative to reduce our API costs.",
    "example": "Meta's Llama 2 is an open source LLM that companies can run on their own servers.",
    "relatedTermIds": ["llm", "fine-tuning"],
    "relatedMilestoneIds": ["E2023_LLAMA", "E2022_BLOOM", "E2022_STABLE_DIFFUSION_RELEASE"],
    "category": "business_term"
  },
  {
    "id": "parameter",
    "term": "Parameter (AI)",
    "shortDefinition": "A number in a neural network that gets adjusted during training to improve performance.",
    "fullDefinition": "Parameters are the numbers inside neural networks that determine how inputs are transformed into outputs. During training, these parameters are adjusted to minimize errors. Model size is often measured in parameters: GPT-3 has 175 billion parameters.",
    "businessContext": "Parameter count is a rough proxy for model capability. Larger models are generally more capable but also more expensive to run. Understanding parameters helps you interpret model comparisons.",
    "inMeetingExample": "We're evaluating a 7B parameter model that should run on our existing hardware.",
    "example": "GPT-4 is rumored to have over 1 trillion parameters across its mixture-of-experts architecture.",
    "relatedTermIds": ["neural-network", "training", "scaling-laws"],
    "relatedMilestoneIds": ["E2020_GPT3"],
    "category": "technical_term"
  },
  {
    "id": "api-first-ai",
    "term": "API-First AI",
    "shortDefinition": "AI capabilities accessed through web APIs rather than running models locally.",
    "fullDefinition": "API-first AI means using AI capabilities by sending requests to remote servers that run the models, rather than deploying models yourself. You send data, get results back, and pay per use. This approach democratized access to powerful AI starting with GPT-3's API in 2020.",
    "businessContext": "API-first AI lets you use cutting-edge models without ML expertise or GPU infrastructure. Trade-offs include ongoing costs, data privacy considerations, and dependency on the provider's reliability and policies.",
    "inMeetingExample": "We're taking an API-first approach with Claude so we can launch quickly without building ML infrastructure.",
    "example": "Instead of training your own language model, you call OpenAI's API and pay $0.002 per 1K tokens.",
    "relatedTermIds": ["llm", "inference", "token"],
    "relatedMilestoneIds": ["E2020_GPT3_API"],
    "category": "business_term"
  },
  {
    "id": "ai-agents",
    "term": "AI Agents",
    "shortDefinition": "AI systems that autonomously take actions to complete multi-step tasks.",
    "fullDefinition": "AI agents are systems that go beyond answering questions—they can take actions, use tools, and complete tasks autonomously. An agent might search the web, write files, call APIs, or interact with other software to achieve a goal. They combine LLM reasoning with the ability to act on the world.",
    "businessContext": "Agents represent the evolution from AI assistance to AI automation. They can handle customer service, research tasks, and workflow automation, but require careful design around reliability and human oversight.",
    "inMeetingExample": "We're building AI agents that can resolve tier-1 support tickets without human intervention.",
    "example": "A customer service agent that can look up orders, process refunds, and update accounts—not just chat about policies.",
    "relatedTermIds": ["llm", "rag", "prompt-engineering"],
    "relatedMilestoneIds": ["E2024_AI_AGENTS"],
    "category": "core_concept"
  },
  {
    "id": "ai-copilot",
    "term": "AI Copilot",
    "shortDefinition": "An AI assistant integrated into professional tools to augment human work.",
    "fullDefinition": "An AI copilot works alongside you in professional software, suggesting next steps, automating routine tasks, and helping you work faster. Unlike standalone AI chatbots, copilots are embedded in the tools you already use—IDEs, Office apps, design software—and understand your work context.",
    "businessContext": "Copilots change how professionals work by handling routine tasks and suggestions. They're becoming standard in enterprise software, with Microsoft, GitHub, and others embedding AI assistance into everyday tools.",
    "inMeetingExample": "Our developers are using GitHub Copilot, and we're piloting Microsoft 365 Copilot for the rest of the team.",
    "example": "GitHub Copilot suggests code as you type; Microsoft 365 Copilot drafts emails based on meeting notes.",
    "relatedTermIds": ["llm", "enterprise-ai", "prompt-engineering"],
    "relatedMilestoneIds": ["E2021_COPILOT", "E2023_ENTERPRISE_AI"],
    "category": "business_term"
  },
  {
    "id": "enterprise-ai",
    "term": "Enterprise AI",
    "shortDefinition": "AI solutions designed for business use with security, compliance, and integration features.",
    "fullDefinition": "Enterprise AI refers to AI products and platforms designed for organizational use. Unlike consumer AI tools, enterprise AI includes data isolation, access controls, audit logging, compliance certifications, and integration with business systems. Major players include Microsoft's Copilot for Microsoft 365, Google's Duet AI, and AWS Bedrock.",
    "businessContext": "Enterprise AI addresses the gap between exciting AI demos and production business use. It's not just about capability—it's about deploying AI safely within organizational constraints and existing workflows.",
    "inMeetingExample": "We need enterprise AI solutions that integrate with our SSO and meet our compliance requirements.",
    "example": "Microsoft 365 Copilot accesses your SharePoint documents but keeps that data separate from general model training.",
    "relatedTermIds": ["ai-copilot", "rag", "guardrails"],
    "relatedMilestoneIds": ["E2023_ENTERPRISE_AI"],
    "category": "business_term"
  },
  {
    "id": "vector-database",
    "term": "Vector Database",
    "shortDefinition": "A database optimized for storing and searching AI embeddings (numerical representations of content).",
    "fullDefinition": "Vector databases store embeddings—numerical representations of text, images, or other data—and enable fast similarity search. When you ask 'find documents similar to this query,' a vector database finds the closest matches by comparing embeddings. This is the foundation of RAG (Retrieval-Augmented Generation) systems.",
    "businessContext": "Vector databases power most enterprise AI search and knowledge systems. Choosing the right one affects cost, performance, and scalability. Options range from managed services (Pinecone) to open-source (Weaviate, Chroma) to database extensions (pgvector).",
    "inMeetingExample": "We're evaluating Pinecone versus pgvector for our RAG implementation.",
    "example": "When you ask a company chatbot a question, it uses a vector database to find relevant internal documents before generating an answer.",
    "relatedTermIds": ["embedding", "rag", "semantic-search"],
    "relatedMilestoneIds": ["E2024_RAG_ADOPTION"],
    "category": "technical_term"
  },
  {
    "id": "guardrails",
    "term": "Guardrails (AI)",
    "shortDefinition": "Safety controls that constrain AI system behavior to prevent harmful or off-topic outputs.",
    "fullDefinition": "AI guardrails are technical and procedural controls that keep AI systems within acceptable boundaries. They can prevent the AI from discussing certain topics, generating harmful content, taking unauthorized actions, or revealing sensitive information. Guardrails are essential for production AI deployments.",
    "businessContext": "Without guardrails, AI systems can embarrass your company, leak data, or take unintended actions. Implementing effective guardrails is a key part of responsible AI deployment and often a compliance requirement.",
    "inMeetingExample": "We need to implement guardrails to ensure the chatbot doesn't discuss competitor products or make promises we can't keep.",
    "example": "A customer service AI has guardrails preventing it from agreeing to refunds over $100 without human approval.",
    "relatedTermIds": ["alignment", "ai-safety", "enterprise-ai"],
    "relatedMilestoneIds": ["E2022_CONSTITUTIONAL_AI", "E2024_AI_AGENTS"],
    "category": "business_term"
  }
]
